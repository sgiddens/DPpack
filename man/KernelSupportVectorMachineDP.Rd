% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StatModels.R
\name{KernelSupportVectorMachineDP}
\alias{KernelSupportVectorMachineDP}
\title{Privacy-preserving Nonlinear Kernel Support Vector Machine}
\description{
This class implements a differentially private support vector
machine with a nonlinear kernel using the objective perturbation technique
\insertCite{chaudhuri2011}{DPpack}.
}
\details{
A new model object of this class accepts as inputs various functions
and hyperparameters related to the specific regression problem. The model
can then be fit with a dataset X (given as a data.frame), a set of binary
labels y for each row of X, as well as upper and lower bounds on the
possible values for each column of X and for y. The algorithm first
transforms the data using a kernel approximation method, then runs the
privacy-preserving linear SVM model on the results
(\code{\link{SupportVectorMachineDP}}). In fitting the model, the class
stores a method \code{XtoV} that transforms additional input data X into
the new dimension, as well as a vector of coefficients coeff for the
transformed data V which satisfy epsilon-level differential privacy. These
can be released directly, or used in conjunction with the predict method to
predict the label of new datapoints.

Note that in order to guarantee epsilon-level privacy for the empirical
risk minimization model, certain constraints must be satisfied for the
values used to construct the object, as well as for the data used to fit.
First, the loss function is assumed to be doubly differentiable. The hinge
loss, which is typically used for linear support vector machines, is not
differentiable at 1. Thus, to satisfy this constraint, this class utilizes
the Huber loss, a smooth approximation to the hinge loss. The level of
approximation to the hinge loss is determined by a user-specified constant,
h, which defaults to 1. Additionally, the regularizer must be 1-strongly
convex and doubly differentiable.

The labels y are assumed to be either -1 or 1. If different values are
provided, they are coerced to be either -1 or 1 prior to fitting the model.
Values in y that are \eqn{\le} 0 are assigned to be -1, while values in y
\eqn{>} 0 are assigned to be 1. Accordingly, new predicted labels are
output as either -1 or 1.
}
\examples{

## ------------------------------------------------
## Method `KernelSupportVectorMachineDP$new`
## ------------------------------------------------

# Construct object for nonlinear SVM
regularizer <- 'l2' # Alternatively, function(coeff) coeff\%*\%coeff/2
eps <- 1
lambda <- 0.1
D <- 20
sampling <- "Gaussian"
regularizer.gr <- function(coeff) coeff # If function given for regularizer
huber.h <- 1
ksvmdp <- KernelSupportVectorMachineDP$new(regularizer, eps, lambda, D,
                                  sampling, regularizer.gr, huber.h)


## ------------------------------------------------
## Method `KernelSupportVectorMachineDP$predict`
## ------------------------------------------------

# Assume Xtest is a new dataframe of the same form as X from fit
# method example, with true labels ytest
# Also assume ksvmdp$fit() has already been run on training data
predicted.y <- ksvmdp$predict(Xtest)
n.errors <- sum(predicted.y!=ytest)

}
\references{
\insertRef{chaudhuri2011}{DPpack}
}
\section{Super classes}{
\code{\link[DPpack:EmpiricalRiskMinimizationDP.CMS]{DPpack::EmpiricalRiskMinimizationDP.CMS}} -> \code{\link[DPpack:SupportVectorMachineDP]{DPpack::SupportVectorMachineDP}} -> \code{KernelSupportVectorMachineDP}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{KernelSupportVectorMachineDP$new()}}
\item \href{#method-XtoV}{\code{KernelSupportVectorMachineDP$XtoV()}}
\item \href{#method-predict}{\code{KernelSupportVectorMachineDP$predict()}}
\item \href{#method-clone}{\code{KernelSupportVectorMachineDP$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="DPpack" data-topic="EmpiricalRiskMinimizationDP.CMS" data-id="fit">}\href{../../DPpack/html/EmpiricalRiskMinimizationDP.CMS.html#method-fit}{\code{DPpack::EmpiricalRiskMinimizationDP.CMS$fit()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new KernelSupportVectorMachineDP object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{KernelSupportVectorMachineDP$new(
  regularizer,
  eps,
  lambda,
  D,
  sampling = "Gaussian",
  regularizer.gr = NULL,
  huber.h = 1,
  phi = NULL,
  gamma = 1
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{regularizer}}{String or regularization function. If a string, must be
'l2', indicating to use l2 regularization. If a function, must have form
as given in regularizer field description. Additionally, in order to
ensure differential privacy, the function must be 1-strongly convex and
doubly differentiable.}

\item{\code{eps}}{Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.}

\item{\code{lambda}}{Nonnegative real number representing the regularization
constant.}

\item{\code{D}}{Nonnegative integer indicating the dimensionality of the transform
space approximating the kernel. Higher values of D provide better kernel
approximations at a cost of computational efficiency.}

\item{\code{sampling}}{String or sampling function. If a string, must be
'Gaussian' (default), indicating to use the sampling function
corresponding to the Gaussian (radial) kernel approximation. If a
function, must take as input a dimension d and return a (d+1)-dimensional
vector of samples corresponding to the Fourier transform of the kernel to
be approximated.}

\item{\code{regularizer.gr}}{Optional function representing the gradient of the
regularizer function function with respect to coeff. Must have form as
given in regularizer.gr field description for parent class. If not given,
gradients are not used to compute the coefficient values in fitting the
model.}

\item{\code{huber.h}}{Positive real number indicating the degree to which the
Huber loss approximates the hinge loss. A smaller value indicates closer
resemblance to the hinge loss, but a larger value of the constant c used
in the objective perturbation algorithm, meaning more noise needs to be
added to ensure differential privacy. Conversely, larger values for this
parameter represent looser approximations to the hinge loss, but smaller
c values and less noise to ensure privacy. Defaults to 1.}

\item{\code{phi}}{Function or NULL (default). If sampling is given as one of the
predefined strings, this input is unnecessary. If sampling is a function,
this should also be a function that takes as inputs an individual row of
of the original dataset x, and a (d+1)-dimensional vector theta sampled
from the Fourier transform of the kernel to be approximated, where d is
the dimension of x. The function then outputs a numeric scalar
corresponding to the pre-filtered value at the given row with the given
sampled vector.}

\item{\code{gamma}}{Positive real number corresponding to the Gaussian kernel
parameter.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new KernelSupportVectorMachineDP object.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Construct object for nonlinear SVM
regularizer <- 'l2' # Alternatively, function(coeff) coeff\%*\%coeff/2
eps <- 1
lambda <- 0.1
D <- 20
sampling <- "Gaussian"
regularizer.gr <- function(coeff) coeff # If function given for regularizer
huber.h <- 1
ksvmdp <- KernelSupportVectorMachineDP$new(regularizer, eps, lambda, D,
                                  sampling, regularizer.gr, huber.h)

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-XtoV"></a>}}
\if{latex}{\out{\hypertarget{method-XtoV}{}}}
\subsection{Method \code{XtoV()}}{
Convert input data X into transformed data V. Uses sampled
pre-filter values and the provided mapping function to produce
D-dimensional data V on which to train the model or predict future
values.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{KernelSupportVectorMachineDP$XtoV(X)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{Matrix corresponding to the original dataset.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Matrix V of size n by D representing the transformed dataset, where
n is the number of rows of X, and D is the provided transformed space
dimension.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-predict"></a>}}
\if{latex}{\out{\hypertarget{method-predict}{}}}
\subsection{Method \code{predict()}}{
Predict label(s) for given X using the XtoV method and fitted
coefficients.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{KernelSupportVectorMachineDP$predict(X, add.bias = FALSE, raw.value = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{Dataframe of data on which to make predictions. Must be of same
form as X used to fit coefficients.}

\item{\code{add.bias}}{Boolean indicating whether to add a bias term to X.
Defaults to FALSE. If add.bias was set to TRUE when fitting the
coefficients, add.bias should be set to TRUE for predictions.}

\item{\code{raw.value}}{Boolean indicating whether to return the raw predicted
value or the rounded class label. If FALSE (default), rounds the values
to -1 or 1. If TRUE, returns the raw score from the SVM model.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Matrix of predicted labels corresponding to each row of X.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assume Xtest is a new dataframe of the same form as X from fit
# method example, with true labels ytest
# Also assume ksvmdp$fit() has already been run on training data
predicted.y <- ksvmdp$predict(Xtest)
n.errors <- sum(predicted.y!=ytest)

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{KernelSupportVectorMachineDP$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
