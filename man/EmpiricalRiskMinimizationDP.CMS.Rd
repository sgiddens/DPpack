% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StatModels.R
\name{EmpiricalRiskMinimizationDP.CMS}
\alias{EmpiricalRiskMinimizationDP.CMS}
\title{Privacy-preserving Empirical Risk Minimization for Binary Classification}
\description{
This class implements differentially private empirical risk
minimization using the objective perturbation technique
\insertCite{chaudhuri2011}{DPpack}. It is intended to be a framework for
building more specific models via inheritance. See
\code{\link{LogisticRegressionDP}} for an example of this type of
structure.
}
\details{
A new model object of this class accepts as inputs a hypothesis
function, a loss function, a regularizer, an epsilon value for differential
privacy, a lambda value that scales the regularizer, and a constant c
meeting certain constraints related to the loss function. The model can
then be fit with a dataset X (given as a data.frame), a set of binary
labels y for each row of X, as well as upper and lower bounds on the
possible values for each column of X and for y. In fitting, the model
stores a vector of coefficients coeff which satisfy epsilon-level
differential privacy. These can be released directly, or used in
conjunction with the predict method to predict the label of new datapoints.

Note that in order to guarantee epsilon-level privacy for the empirical
risk minimization model, certain constraints must be satisfied for the
values used to construct the object, as well as for the data used to fit.
Specifically, the provided loss function must be convex and doubly
differentiable w.r.t. y.hat with |loss'(y.hat,y)|<=1 and
|loss''(y.hat,y)|<=c for some constant c and for all possible values of
y.hat and y, where y.hat is the predicted label and y is the true label.
Additionally, it is assumed that if x represents a single row of the
dataset X, then ||x||<=1 for all x. Note that because of this, a bias term
cannot be included without appropriate scaling/preprocessing of the
dataset. To ensure privacy, the add.bias argument in the $fit and $predict
methods should only be utilized in subclasses within this package, not in
this class.
}
\examples{

## ------------------------------------------------
## Method `EmpiricalRiskMinimizationDP.CMS$new`
## ------------------------------------------------

# Construct object for logistic regression
h <- function(X, coeff) e1071::sigmoid(X\%*\%coeff)
# Cross entropy loss
loss <- function(y.hat,y) -(y*log(y.hat) + (1-y)*log(1-y.hat))
regularizer <- 'l2' # Alternatively, function(coeff) coeff\%*\%coeff/2
eps <- 1
lambda <- 0.1
c <- 1/4 # Required value for logistic regression
h.gr <- function(X, coeff) as.numeric(e1071::dsigmoid(X\%*\%coeff))*t(X)
loss.gr <- function(y.hat, y) -y/y.hat + (1-y)/(1-y.hat)
regularizer.gr <- function(coeff) coeff
ermdp <- EmpiricalRiskMinimizationDP.CMS$new(h, loss, regularizer, eps,
                                             lambda, c, h.gr, loss.gr,
                                             regularizer.gr)


## ------------------------------------------------
## Method `EmpiricalRiskMinimizationDP.CMS$fit`
## ------------------------------------------------

# Assume X is dataframe meeting assumptions for privacy
# Assume 2 columns of X each bounded between -1 and 1
# Assume y is 0 or 1 labels for each row of X
# Assume ermdp is previously constructed object as in $new example
upper.bounds <- c( 1, 1, 1) # Bounds for X and y
lower.bounds <- c(-1,-1, 0) # Bounds for X and y
ermdp$fit(X, y, upper.bounds, lower.bounds)
ermdp$coeff # Gets private coefficients


## ------------------------------------------------
## Method `EmpiricalRiskMinimizationDP.CMS$predict`
## ------------------------------------------------

# Assume Xtest is a new dataframe of the same form as X from fit
# method example, with true labels ytest
# Also assume ermdp$fit() has already been run on training data
predicted.y <- ermdp$predict(Xtest) # Note these values need to be rounded
n.errors <- sum(abs(round(predicted.y)-ytest))

}
\references{
\insertRef{chaudhuri2011}{DPpack}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{h}}{Hypothesis function of the form h(X, coeff), where X is a matrix
and coeff is a vector or matrix, that returns a column matrix of
predicted labels for each row of X.}

\item{\code{h.gr}}{Function representing the gradient of the hypothesis function
with respect to the values in coeff and of the same form as h. Should be
given such that the ith row of the output represents the gradient of h
with respect to the ith coefficient.}

\item{\code{loss}}{Loss function of the form loss(y.hat, y), where y.hat and y are
matrices, that returns a matrix of the same shape as y.hat and y of loss
function values for the empirical risk minimization model with predicted
labels y.hat and true labels y.}

\item{\code{loss.gr}}{Function representing the gradient of the loss function with
respect to y.hat and of the same form as loss. Should be given such that
the ith row of the output represents the gradient of loss at the ith set
of input values.}

\item{\code{regularizer}}{Regularization function. Must be of the form
regularizer(coeff), where coeff is a vector or matrix, that returns the
value of the regularizer at coeff.}

\item{\code{regularizer.gr}}{Function representing the gradient of the
regularization function with respect to coeff and of the same form as
regularizer. Should return a vector. If regularizer is a string, this
value is ignored.}

\item{\code{lambda}}{Nonnegative real number representing the regularization
constant.}

\item{\code{eps}}{Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.}

\item{\code{c}}{Positive real number denoting the upper bound on the absolute
value of the second derivative of the loss function, as required to
ensure differential privacy.}

\item{\code{coeff}}{Numeric vector of coefficients for the model.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{EmpiricalRiskMinimizationDP.CMS$new()}}
\item \href{#method-fit}{\code{EmpiricalRiskMinimizationDP.CMS$fit()}}
\item \href{#method-predict}{\code{EmpiricalRiskMinimizationDP.CMS$predict()}}
\item \href{#method-clone}{\code{EmpiricalRiskMinimizationDP.CMS$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new EmpiricalRiskMinimizationDP.CMS object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{EmpiricalRiskMinimizationDP.CMS$new(
  h,
  loss,
  regularizer,
  eps,
  lambda,
  c,
  h.gr = NULL,
  loss.gr = NULL,
  regularizer.gr = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{h}}{Hypothesis function. Must have form as given in h field
description.}

\item{\code{loss}}{Loss function. Must have form as given in loss field
description. Additionally, in order to ensure differential privacy, the
function must be convex and doubly differentiable w.r.t. y.hat with
|loss'(y.hat, y)|<=1 and |loss''(y.hat, y)|<=c for some constant c and
for all possible values of y.hat and y.}

\item{\code{regularizer}}{String or regularization function. If a string, must be
'l2', indicating to use l2 regularization. If a function, must have form
as given in regularizer field description. Additionally, in order to
ensure differential privacy, the function must be 1-strongly convex and
doubly differentiable.}

\item{\code{eps}}{Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.}

\item{\code{lambda}}{Nonnegative real number representing the regularization
constant.}

\item{\code{c}}{Positive real number such that |loss''(y.hat,y)|<=c for all
possible values of y.hat and y.}

\item{\code{h.gr}}{Optional function representing the gradient of the hypothesis
function with respect to the values in coeff. Must have form as given in
h.gr field description. If not given, gradients are not used to compute
the coefficient values in fitting the model.}

\item{\code{loss.gr}}{Optional function representing the gradient of the loss
function with respect to y.hat. Must have form as given in loss.gr field
description. If not given, gradients are not used to compute the
coefficient values in fitting the model.}

\item{\code{regularizer.gr}}{Optional function representing the gradient of the
regularizer function function with respect to coeff. Must have form as
given in regularizer.gr field description. If not given, gradients are
not used to compute the coefficient values in fitting the model.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new EmpiricalRiskMinimizationDP.CMS object.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Construct object for logistic regression
h <- function(X, coeff) e1071::sigmoid(X\%*\%coeff)
# Cross entropy loss
loss <- function(y.hat,y) -(y*log(y.hat) + (1-y)*log(1-y.hat))
regularizer <- 'l2' # Alternatively, function(coeff) coeff\%*\%coeff/2
eps <- 1
lambda <- 0.1
c <- 1/4 # Required value for logistic regression
h.gr <- function(X, coeff) as.numeric(e1071::dsigmoid(X\%*\%coeff))*t(X)
loss.gr <- function(y.hat, y) -y/y.hat + (1-y)/(1-y.hat)
regularizer.gr <- function(coeff) coeff
ermdp <- EmpiricalRiskMinimizationDP.CMS$new(h, loss, regularizer, eps,
                                             lambda, c, h.gr, loss.gr,
                                             regularizer.gr)

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-fit"></a>}}
\if{latex}{\out{\hypertarget{method-fit}{}}}
\subsection{Method \code{fit()}}{
Fit the differentially private emprirical risk minimization
model. The function runs the objective perturbation algorithm
\insertCite{chaudhuri2011}{DPpack} to generate an objective function. A
numerical optimization method is then run to find optimal coefficients
for fitting the model given the training data and hyperparameters. The
built-in \code{\link{optim}} function using the "BFGS" optimization
method is used. If h.gr, loss.gr, and regularizer.gr are all given in the
construction of the object, the gradient of the objective function is
utilized by optim as well. The resulting privacy-preserving coefficients
are stored in coeff.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{EmpiricalRiskMinimizationDP.CMS$fit(
  X,
  y,
  upper.bounds = NULL,
  lower.bounds = NULL,
  add.bias = FALSE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{Dataframe of data to be fit.}

\item{\code{y}}{Vector or matrix of true labels for each row of X.}

\item{\code{upper.bounds}}{Optional vector of length ncol(X)+1 giving upper bounds
on the values in each column of X and the values in y. The last value in
the vector is assumed to be the upper bound on y, while the first ncol(X)
values are assumed to be in the same order as the corresponding columns
of X. If NULL (default), the values are computed to be the maximum values
in the data, which results in additional privacy loss. Any value in the
columns of X and y larger than the corresponding upper bound is clipped
at the bound.}

\item{\code{lower.bounds}}{Optional vector of length ncol(X)+1 giving lower bounds
on the values in each column of X and the values in y. The last value in
the vector is assumed to be the lower bound on y, while the first ncol(X)
values are assumed to be in the same order as the corresponding columns
of X. If NULL (default), the values are computed to be the minimum values
in the data, which results in additional privacy loss. Any value in the
columns of X and y smaller than the corresponding lower bound is clipped
at the bound.}

\item{\code{add.bias}}{Boolean indicating whether to add a bias term to X.
Defaults to FALSE.}
}
\if{html}{\out{</div>}}
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assume X is dataframe meeting assumptions for privacy
# Assume 2 columns of X each bounded between -1 and 1
# Assume y is 0 or 1 labels for each row of X
# Assume ermdp is previously constructed object as in $new example
upper.bounds <- c( 1, 1, 1) # Bounds for X and y
lower.bounds <- c(-1,-1, 0) # Bounds for X and y
ermdp$fit(X, y, upper.bounds, lower.bounds)
ermdp$coeff # Gets private coefficients

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-predict"></a>}}
\if{latex}{\out{\hypertarget{method-predict}{}}}
\subsection{Method \code{predict()}}{
Predict label(s) for given X using the fitted coefficients.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{EmpiricalRiskMinimizationDP.CMS$predict(X, add.bias = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{Dataframe of data on which to make predictions. Must be of same
form as X used to fit coefficients.}

\item{\code{add.bias}}{Boolean indicating whether to add a bias term to X.
Defaults to FALSE. If add.bias was set to TRUE when fitting the
coefficients, add.bias should be set to TRUE for predictions.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Matrix of predicted labels corresponding to each row of X.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assume Xtest is a new dataframe of the same form as X from fit
# method example, with true labels ytest
# Also assume ermdp$fit() has already been run on training data
predicted.y <- ermdp$predict(Xtest) # Note these values need to be rounded
n.errors <- sum(abs(round(predicted.y)-ytest))

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{EmpiricalRiskMinimizationDP.CMS$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
