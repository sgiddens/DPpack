% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StatModels.R
\name{SupportVectorMachineDP}
\alias{SupportVectorMachineDP}
\title{Privacy-preserving Support Vector Machine}
\description{
This class implements a differentially private support vector
machine (SVM) using the objective perturbation technique
\insertCite{chaudhuri2011}{DPpack}.
}
\details{
A new model object of this class accepts as inputs a regularizer, an
epsilon value for differential privacy, and a lambda value that scales the
regularizer. The model can then be fit with a dataset X (given as a
data.frame), a set of binary labels y for each row of X, as well as upper
and lower bounds on the possible values for each column of X and for y. In
fitting, the model stores a vector of coefficients coeff which satisfy
epsilon-level differential privacy. These can be released directly, or used
in conjunction with the predict method to predict the label of new
datapoints.

Note that in order to guarantee epsilon-level privacy for the empirical
risk minimization model, certain constraints must be satisfied for the
values used to construct the object, as well as for the data used to fit.
First, the loss function is assumed to be doubly differentiable. The hinge
loss, which is typically used for linear support vector machines, is not
differentiable at 1. Thus, to satisfy this constraint, this class utilizes
the Huber loss, a smooth approximation to the hinge loss. The level of
approximation to the hinge loss is determined by a user-specified constant,
h, which defaults to 1. Additionally, the regularizer must be 1-strongly
convex and doubly differentiable. Finally, it is assumed that if x
represents a single row of the dataset X, then \eqn{||x||\le 1} for all
\eqn{x}. In order to ensure this constraint is satisfied, the dataset is
preprocessed using provided upper and lower bounds on the columns of X to
scale the values in such a way that this constraint is met. After the
private coefficients are generated, these are then postprocessed and
un-scaled so that the stored coefficients correspond to the original data.
This does not result in additional privacy loss as long as the upper and
lower bounds provided when fitting the model do not depend directly on the
data. Due to this constraint on \eqn{x}, it is best to avoid using a bias
term in the model whenever possible. If a bias term must be used, the issue
can be partially circumvented by adding a constant column to X before
fitting the model, which will be scaled along with the rest of X. The
\code{fit} method contains functionality to add a column of constant 1s to
X before scaling, if desired.

The preprocessing of X is done as follows. First, the largest in absolute
value of the upper and lower bounds on each column are used to scale each
column individually such that the largest value in each column is at most 1
in absolute value. Second, each value in X is divided by the square root of
the number of predictors of X (including bias term). These two scalings
ensure that each row of X satisfies the necessary constraints for
differential privacy. Additionally, the labels y are assumed to be either
-1 or 1. If different values are provided, they are coerced to be either -1
or 1 prior to fitting the model. Values in y that are \eqn{\le} 0 are
assigned to be -1, while values in y \eqn{>} 0 are assigned to be 1.
Accordingly, new predicted labels are output as either -1 or 1.
}
\examples{

## ------------------------------------------------
## Method `SupportVectorMachineDP$new`
## ------------------------------------------------

# Construct object for SVM
regularizer <- 'l2' # Alternatively, function(coeff) coeff\%*\%coeff/2
eps <- 1
lambda <- 0.1
regularizer.gr <- function(coeff) coeff # If function given for regularizer
huber.h <- 1
svmdp <- SupportVectorMachineDP$new(regularizer, eps, lambda,
                                  regularizer.gr=regularizer.gr,
                                  huber.h=huber.h)


## ------------------------------------------------
## Method `SupportVectorMachineDP$predict`
## ------------------------------------------------

# Assume Xtest is a new dataframe of the same form as X from fit
# method example, with true labels ytest
# Also assume svmdp$fit() has already been run on training data
predicted.y <- svmdp$predict(Xtest)
n.errors <- sum(predicted.y!=ytest)

}
\references{
\insertRef{chaudhuri2011}{DPpack}
}
\section{Super class}{
\code{\link[DPpack:EmpiricalRiskMinimizationDP.CMS]{DPpack::EmpiricalRiskMinimizationDP.CMS}} -> \code{SupportVectorMachineDP}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{SupportVectorMachineDP$new()}}
\item \href{#method-XtoV}{\code{SupportVectorMachineDP$XtoV()}}
\item \href{#method-predict}{\code{SupportVectorMachineDP$predict()}}
\item \href{#method-clone}{\code{SupportVectorMachineDP$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="DPpack" data-topic="EmpiricalRiskMinimizationDP.CMS" data-id="fit">}\href{../../DPpack/html/EmpiricalRiskMinimizationDP.CMS.html#method-fit}{\code{DPpack::EmpiricalRiskMinimizationDP.CMS$fit()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new SupportVectorMachineDP object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{SupportVectorMachineDP$new(
  regularizer,
  eps,
  lambda,
  kernel = "linear",
  D = NULL,
  gamma = 1,
  regularizer.gr = NULL,
  huber.h = 1
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{regularizer}}{String or regularization function. If a string, must be
'l2', indicating to use l2 regularization. If a function, must have form
as given in regularizer field description. Additionally, in order to
ensure differential privacy, the function must be 1-strongly convex and
doubly differentiable.}

\item{\code{eps}}{Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.}

\item{\code{lambda}}{Nonnegative real number representing the regularization
constant.}

\item{\code{kernel}}{String indicating which kernel to use for SVM. Must be one of
{'linear', 'Gaussian'}. If 'linear' (default), linear SVM is used. If
'Gaussian,' uses the sampling function corresponding to the Gaussian
(radial) kernel approximation.}

\item{\code{D}}{Nonnegative integer indicating the dimensionality of the transform
space approximating the kernel if a nonlinear kernel is used. Higher
values of D provide better kernel approximations at a cost of
computational efficiency. Defaults to NULL.}

\item{\code{gamma}}{Positive real number corresponding to the Gaussian kernel
parameter. Defaults to 1.}

\item{\code{regularizer.gr}}{Optional function representing the gradient of the
regularizer function function with respect to coeff. Must have form as
given in regularizer.gr field description for parent class. If not given,
gradients are not used to compute the coefficient values in fitting the
model.}

\item{\code{huber.h}}{Positive real number indicating the degree to which the
Huber loss approximates the hinge loss. A smaller value indicates closer
resemblance to the hinge loss, but a larger value of the constant c used
in the objective perturbation algorithm, meaning more noise needs to be
added to ensure differential privacy. Conversely, larger values for this
parameter represent looser approximations to the hinge loss, but smaller
c values and less noise to ensure privacy. Defaults to 1.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new SupportVectorMachineDP object.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Construct object for SVM
regularizer <- 'l2' # Alternatively, function(coeff) coeff\%*\%coeff/2
eps <- 1
lambda <- 0.1
regularizer.gr <- function(coeff) coeff # If function given for regularizer
huber.h <- 1
svmdp <- SupportVectorMachineDP$new(regularizer, eps, lambda,
                                  regularizer.gr=regularizer.gr,
                                  huber.h=huber.h)

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-XtoV"></a>}}
\if{latex}{\out{\hypertarget{method-XtoV}{}}}
\subsection{Method \code{XtoV()}}{
Convert input data X into transformed data V. Uses sampled
pre-filter values and the provided mapping function to produce
D-dimensional data V on which to train the model or predict future
values.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{SupportVectorMachineDP$XtoV(X)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{Matrix corresponding to the original dataset.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Matrix V of size n by D representing the transformed dataset, where
n is the number of rows of X, and D is the provided transformed space
dimension.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-predict"></a>}}
\if{latex}{\out{\hypertarget{method-predict}{}}}
\subsection{Method \code{predict()}}{
Predict label(s) for given X using the fitted coefficients.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{SupportVectorMachineDP$predict(X, add.bias = FALSE, raw.value = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{Dataframe of data on which to make predictions. Must be of same
form as X used to fit coefficients.}

\item{\code{add.bias}}{Boolean indicating whether to add a bias term to X.
Defaults to FALSE. If add.bias was set to TRUE when fitting the
coefficients, add.bias should be set to TRUE for predictions.}

\item{\code{raw.value}}{Boolean indicating whether to return the raw predicted
value or the rounded class label. If FALSE (default), rounds the values
to -1 or 1. If TRUE, returns the raw score from the SVM model.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Matrix of predicted labels corresponding to each row of X.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assume Xtest is a new dataframe of the same form as X from fit
# method example, with true labels ytest
# Also assume svmdp$fit() has already been run on training data
predicted.y <- svmdp$predict(Xtest)
n.errors <- sum(predicted.y!=ytest)

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{SupportVectorMachineDP$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
