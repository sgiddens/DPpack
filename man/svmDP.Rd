% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StatModels.R
\name{svmDP}
\alias{svmDP}
\title{Privacy-preserving Support Vector Machine}
\description{
This class implements a differentially private support vector
machine (SVM) using the objective perturbation technique
\insertCite{chaudhuri2011}{DPpack}.
}
\details{
After constructing an \code{svmDP} object, the object can be fit to
a provided dataset. In fitting, the model stores a vector of coefficients
\code{coeff} which satisfy epsilon-level differential privacy.
Additionally, if a nonlinear kernel is chosen, the models stores a mapping
function from the input data X to a higher dimensional dataset V in the
form of a method \code{XtoV} as required by
\insertCite{chaudhuri2011}{DPpack}. These can be released directly, or used
in conjunction with the predict method to privately predict the label of
new datapoints.

Note that in order to guarantee epsilon-level privacy for the empirical
risk minimization model, certain constraints must be satisfied for the
values used to construct the object, as well as for the data used to fit.
First, the loss function is assumed to be doubly differentiable. The hinge
loss, which is typically used for support vector machines, is not
differentiable at 1. Thus, to satisfy this constraint, this class utilizes
the Huber loss, a smooth approximation to the hinge loss
\insertCite{Chapelle2007}{DPpack}. The level of approximation to the hinge
loss is determined by a user-specified constant, h, which defaults to 0.5,
a typical value. Additionally, the regularizer must be 1-strongly convex
and doubly differentiable. Finally, it is assumed that if x represents a
single row of the dataset X, then the l2-norm of x is at most 1 for all x.
In order to ensure this constraint is satisfied, the dataset is
preprocessed and scaled, and the resulting coefficients are postprocessed
and un-scaled so that the stored coefficients correspond to the original
data. Due to this constraint on x, it is best to avoid using a bias term in
the model whenever possible. If a bias term must be used, the issue can be
partially circumvented by adding a constant column to X before fitting the
model, which will be scaled along with the rest of X. The \code{fit} method
contains functionality to add a column of constant 1s to X before scaling,
if desired.
}
\examples{

## ------------------------------------------------
## Method `svmDP$new`
## ------------------------------------------------

# Construct object for SVM
regularizer <- 'l2' # Alternatively, function(coeff) coeff\%*\%coeff/2
eps <- 1
lambda <- 0.1
regularizer.gr <- function(coeff) coeff # If function given for regularizer
huber.h <- 0.5
svmdp <- svmDP$new(regularizer, eps, lambda,
                                  regularizer.gr=regularizer.gr,
                                  huber.h=huber.h)


## ------------------------------------------------
## Method `svmDP$predict`
## ------------------------------------------------

# Assume Xtest is a new dataframe of the same form as X from fit
# method example, with true labels ytest
# Also assume svmdp$fit() has already been run on training data
predicted.y <- svmdp$predict(Xtest)
n.errors <- sum(predicted.y!=ytest)

}
\references{
\insertRef{chaudhuri2011}{DPpack}

\insertRef{Chapelle2007}{DPpack}
}
\section{Super class}{
\code{\link[DPpack:EmpiricalRiskMinimizationDP.CMS]{DPpack::EmpiricalRiskMinimizationDP.CMS}} -> \code{svmDP}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{svmDP$new()}}
\item \href{#method-XtoV}{\code{svmDP$XtoV()}}
\item \href{#method-predict}{\code{svmDP$predict()}}
\item \href{#method-clone}{\code{svmDP$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="DPpack" data-topic="EmpiricalRiskMinimizationDP.CMS" data-id="fit">}\href{../../DPpack/html/EmpiricalRiskMinimizationDP.CMS.html#method-fit}{\code{DPpack::EmpiricalRiskMinimizationDP.CMS$fit()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new \code{svmDP} object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{svmDP$new(
  regularizer,
  eps,
  lambda,
  kernel = "linear",
  D = NULL,
  gamma = 1,
  regularizer.gr = NULL,
  huber.h = 0.5
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{regularizer}}{String or regularization function. If a string, must be
'l2', indicating to use l2 regularization. If a function, must have form
\code{regularizer(coeff)}, where \code{coeff} is a vector or matrix, and
return the value of the regularizer at \code{coeff}. See
\code{\link{regularizer.l2}} for an example. Additionally, in order to
ensure differential privacy, the function must be 1-strongly convex and
doubly differentiable.}

\item{\code{eps}}{Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.}

\item{\code{lambda}}{Nonnegative real number representing the regularization
constant.}

\item{\code{kernel}}{String indicating which kernel to use for SVM. Must be one of
{'linear', 'Gaussian'}. If 'linear' (default), linear SVM is used. If
'Gaussian,' uses the sampling function corresponding to the Gaussian
(radial) kernel approximation.}

\item{\code{D}}{Nonnegative integer indicating the dimensionality of the transform
space approximating the kernel if a nonlinear kernel is used. Higher
values of D provide better kernel approximations at a cost of
computational efficiency. Defaults to NULL.}

\item{\code{gamma}}{Positive real number corresponding to the Gaussian kernel
parameter. Defaults to 1.}

\item{\code{regularizer.gr}}{Optional function representing the gradient of the
regularization function with respect to \code{coeff} and of the form
\code{regularizer.gr(coeff)}. Should return a vector. See
\code{\link{regularizer.gr.l2}} for an example. If \code{regularizer} is
given as a string, this value is ignored. If not given and
\code{regularizer} is a function, non-gradient based optimization methods
are used to compute the coefficient values in fitting the model.}

\item{\code{huber.h}}{Positive real number indicating the degree to which the
Huber loss approximates the hinge loss. Defaults to 0.5
\insertCite{Chapelle2007}{DPpack}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new svmDP object.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Construct object for SVM
regularizer <- 'l2' # Alternatively, function(coeff) coeff\%*\%coeff/2
eps <- 1
lambda <- 0.1
regularizer.gr <- function(coeff) coeff # If function given for regularizer
huber.h <- 0.5
svmdp <- svmDP$new(regularizer, eps, lambda,
                                  regularizer.gr=regularizer.gr,
                                  huber.h=huber.h)

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-XtoV"></a>}}
\if{latex}{\out{\hypertarget{method-XtoV}{}}}
\subsection{Method \code{XtoV()}}{
Convert input data X into transformed data V. Uses sampled
pre-filter values and a mapping function based on the chosen kernel to
produce D-dimensional data V on which to train the model or predict
future values. This method is only used if the kernel is nonlinear. See
\insertCite{chaudhuri2011}{DPpack} for more details.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{svmDP$XtoV(X)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{Matrix corresponding to the original dataset.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Matrix V of size n by D representing the transformed dataset, where
n is the number of rows of X, and D is the provided transformed space
dimension.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-predict"></a>}}
\if{latex}{\out{\hypertarget{method-predict}{}}}
\subsection{Method \code{predict()}}{
Predict label(s) for given \code{X} using the fitted
coefficients.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{svmDP$predict(X, add.bias = FALSE, raw.value = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{Dataframe of data on which to make predictions. Must be of same
form as \code{X} used to fit coefficients.}

\item{\code{add.bias}}{Boolean indicating whether to add a bias term to \code{X}.
Defaults to FALSE. If add.bias was set to TRUE when fitting the
coefficients, add.bias should be set to TRUE for predictions.}

\item{\code{raw.value}}{Boolean indicating whether to return the raw predicted
value or the rounded class label. If FALSE (default), outputs the
predicted labels 0 or 1. If TRUE, returns the raw score from the SVM
model.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Matrix of predicted labels or scores corresponding to each row of
\code{X}.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assume Xtest is a new dataframe of the same form as X from fit
# method example, with true labels ytest
# Also assume svmdp$fit() has already been run on training data
predicted.y <- svmdp$predict(Xtest)
n.errors <- sum(predicted.y!=ytest)

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{svmDP$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
