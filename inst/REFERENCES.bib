@ARTICLE{Liu2019a,
  author={Liu, Fang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={Generalized Gaussian Mechanism for Differential Privacy},
  year={2019},
  volume={31},
  number={4},
  pages={747-756},
  doi={10.1109/TKDE.2018.2845388}}

@InProceedings{Dwork2006a,
author="Dwork, Cynthia
and McSherry, Frank
and Nissim, Kobbi
and Smith, Adam",
editor="Halevi, Shai
and Rabin, Tal",
title="Calibrating Noise to Sensitivity in Private Data Analysis",
booktitle="Theory of Cryptography",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="265--284",
abstract="We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.",
isbn="978-3-540-32732-5"
}

@article{Liu2019b,
  title={Statistical Properties of Sanitized Results from Differentially Private Laplace Mechanism with Univariate Bounding Constraints},
  author={F. Liu},
  journal={Trans. Data Priv.},
  year={2019},
  volume={12},
  pages={169-195}
}

@inproceedings{Smith2011a,
author = {Smith, Adam},
title = {Privacy-Preserving Statistical Estimation with Optimal Convergence Rates},
year = {2011},
isbn = {9781450306911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993636.1993743},
doi = {10.1145/1993636.1993743},
abstract = {Consider an analyst who wants to release aggregate statistics about a data set containing
sensitive information. Using differentially private algorithms guarantees that the
released statistics reveal very little about any particular record in the data set.
In this paper we study the asymptotic properties of differentially private algorithms
for statistical inference.We show that for a large class of statistical estimators
T and input distributions P, there is a differentially private estimator AT with the
same asymptotic distribution as T. That is, the random variables AT(X) and T(X) converge
in distribution when X consists of an i.i.d. sample from P of increasing size. This
implies that AT(X) is essentially as good as the original statistic T(X) for statistical
inference, for sufficiently large samples. Our technique applies to (almost) any pair
T,P such that T is asymptotically normal on i.i.d. samples from P---in particular,
to parametric maximum likelihood estimators and estimators for logistic and linear
regression under standard regularity conditions.A consequence of our techniques is
the existence of low-space streaming algorithms whose output converges to the same
asymptotic distribution as a given estimator T (for the same class of estimators and
input distributions as above).},
booktitle = {Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing},
pages = {813–822},
numpages = {10},
keywords = {statistical inference, differential privacy, asymptotic distribution},
location = {San Jose, California, USA},
series = {STOC '11}
}

@inproceedings{Kifer2011,
author = {Kifer, Daniel and Machanavajjhala, Ashwin},
title = {No Free Lunch in Data Privacy},
year = {2011},
isbn = {9781450306614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1989323.1989345},
doi = {10.1145/1989323.1989345},
abstract = {Differential privacy is a powerful tool for providing privacy-preserving noisy query
answers over statistical databases. It guarantees that the distribution of noisy query
answers changes very little with the addition or deletion of any tuple. It is frequently
accompanied by popularized claims that it provides privacy without any assumptions
about the data and that it protects against attackers who know all but one record.
In this paper we critically analyze the privacy protections offered by differential
privacy.First, we use a no-free-lunch theorem, which defines non-privacy as a game,
to argue that it is not possible to provide privacy and utility without making assumptions
about how the data are generated. Then we explain where assumptions are needed. We
argue that privacy of an individual is preserved when it is possible to limit the
inference of an attacker about the participation of the individual in the data generating
process. This is different from limiting the inference about the presence of a tuple
(for example, Bob's participation in a social network may cause edges to form between
pairs of his friends, so that it affects more than just the tuple labeled as "Bob").
The definition of evidence of participation, in turn, depends on how the data are
generated -- this is how assumptions enter the picture. We explain these ideas using
examples from social network research as well as tabular data for which deterministic
statistics have been previously released. In both cases the notion of participation
varies, the use of differential privacy can lead to privacy breaches, and differential
privacy does not always adequately limit inference about participation.},
booktitle = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
pages = {193–204},
numpages = {12},
keywords = {differential privacy, privacy},
location = {Athens, Greece},
series = {SIGMOD '11}
}

@article{chaudhuri2011,
  author  = {Kamalika Chaudhuri and Claire Monteleoni and Anand D. Sarwate},
  title   = {Differentially Private Empirical Risk Minimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {29},
  pages   = {1069-1109},
  url     = {http://jmlr.org/papers/v12/chaudhuri11a.html}
}


@InProceedings{Kifer2012,
  title = 	 {Private Convex Empirical Risk Minimization and High-dimensional Regression},
  author = 	 {Kifer, Daniel and Smith, Adam and Thakurta, Abhradeep},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {25.1--25.40},
  year = 	 {2012},
  editor = 	 {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/kifer12/kifer12.pdf},
  url = 	 {https://proceedings.mlr.press/v23/kifer12.html},
  abstract = 	 {We consider \emphdifferentially private algorithms for convex empirical risk minimization (ERM). Differential privacy (Dwork et al., 2006b) is a recently introduced notion of privacy which guarantees that an algorithm’s output does not depend on the data of any individual in the dataset. This is crucial in fields that handle sensitive data, such as genomics, collaborative filtering, and economics. Our motivation is the design of private algorithms for sparse learning problems, in which one aims to find solutions (e.g., regression parameters) with few non-zero coefficients. To this end: (a) We significantly extend the analysis of the “objective perturbation” algorithm of Chaudhuri et al. (2011) for convex ERM problems. We show that their method can be modified to use less noise (be more accurate), and to apply to problems with hard constraints and non-differentiable regularizers. We also give a tighter, data-dependent analysis of the additional error introduced by their method. A key tool in our analysis is a new nontrivial limit theorem for differential privacy which is of independent interest: if a sequence of differentially private algorithms converges, in a \emphweak sense, then the limit algorithm is also differentially private. In particular, our methods give the best known algorithms for differentially private linear regression. These methods work in settings where the number of parameters p is less than the number of samples n. (b) We give the first two private algorithms for \emphsparse regression problems in high-dimensional settings, where p is much larger than n. We analyze their performance for linear regression: under standard assumptions on the data, our algorithms have vanishing empirical risk for n = poly(s, \log p) when there exists a good regression vector with s nonzero coefficients. Our algorithms demonstrate that randomized algorithms for sparse regression problems can be both stable and accurate - a combination which is impossible for deterministic algorithms.}
}

